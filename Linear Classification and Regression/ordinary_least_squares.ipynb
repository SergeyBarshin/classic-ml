{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ordinary Least Squares\n",
    "## Метод наименьших квадратов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зададим линейную функцию зависимости наблюдений: $y = w_0 + \\sum_{i=1}^{m}w_ix_i$. Добавим фиктивную размерность $x_0=1$ для каждого наблюдения, тогда формулу можно будет записать $y = \\sum_{i=0}^m w_i x_i = \\vec{w}^T \\vec{x}$.\n",
    "\n",
    "Если рассматривать матрицу наблюдения-признаки, у которой в строках находятся примеры из набора данных, то нам необходимо добавить единичную колонку слева. Зададим модель следующим образом:\n",
    "$$\\large \\vec{y} = X\\vec{w} + \\epsilon,$$\n",
    "где\n",
    "- $\\vec y \\in R^n$ - целевая переменная;\n",
    "- $\\vec \\omega \\in R^{(m+1)}$ - вектор-столбец параметров (в мо называют весами);\n",
    "- $X \\in R^{n \\times (m+1)}$ – матрица наблюдений и признаков размерности $n$ строк на $m + 1$ столбцов (включая фиктивную единичную колонку слева) с полным рангом по столбцам: $\\text{rank}\\left(X\\right) = m + 1$;\n",
    "- $\\epsilon \\in R^n$ -  переменная (вектор-столбец), соотвествующая случайной, непрогнозируемой ошибке модели;\n",
    "\n",
    "Можем записать уравнения для каждого наблюдения:\n",
    "$y_i = \\sum_{j=0}^{m}w_iX_{ij}+\\epsilon_i$\n",
    "\n",
    "Также на модель накладываются следующие ограничения (иначе это будет не линейная регрессия)\n",
    "- Матожидание всех случайных ошибок $\\forall i: \\mathbb{E}\\left[\\epsilon_i\\right] = 0$;\n",
    "- дисперсия случайных ошибок одинакова и кончна $\\forall i: \\text{Var}(\\omega_i)=\\sigma^2 < \\infty$;\n",
    "- случайные ошибки не скоррелированы: $\\forall i \\neq j: \\text{Cov}\\left(\\epsilon_i, \\epsilon_j\\right) = 0$.\n",
    "\n",
    "Оценка $\\hat{w}_i$ весов $w_i$ называется линейной, если $\\large \\hat{w}_i = \\omega_{1i}y_1 + \\omega_{2i}y_2 + \\cdots + \\omega_{ni}y_n,$\n",
    "где $\\forall\\ k\\ \\omega_{ki}$ зависит только от наблюдаемых данных $X$ и почти наверняка нелинейно.\n",
    "\n",
    "Поскольку решениее для поиска оптимальных весов являеся линейная оценка, модель называется линейной регрессией.\n",
    "\n",
    "Один из способов вычислить значения параметров модели является метод наименьших квадратов (МНК), который минимизирует среднеквадратичную ошибку между реальным значением зависимой переменной и прогнозом, выданным моделью:\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\\mathcal{L}\\left(X, \\vec{y}, \\vec{w} \\right) &=& \\frac{1}{2n} \\sum_{i=1}^n \\left(y_i - \\vec{w}^T \\vec{x}_i\\right)^2 \\\\ &=& \\frac{1}{2n} \\left\\| \\vec{y} - X \\vec{w} \\right\\|_2^2 \\\\ &=& \\frac{1}{2n} \\left(\\vec{y} - X \\vec{w}\\right)^T \\left(\\vec{y} - X \\vec{w}\\right) \\end{array}$$\n",
    "\n",
    "Для решения этой задачи оптимизации необходимо вычислить производные по параметрам модели, приравнять их у нулю и решить полученные уравнения отностительно $\\vec w$.\n",
    "\n",
    "$$\\large \\begin{array}{rcl} \\frac{\\partial \\mathcal{L}}{\\partial \\vec{w}} &=& \\frac{\\partial}{\\partial \\vec{w}} \\frac{1}{2n} \\left( \\vec{y}^T \\vec{y} -2\\vec{y}^T X \\vec{w} + \\vec{w}^T X^T X \\vec{w}\\right) \\\\ &=& \\frac{1}{2n} \\left(-2 X^T \\vec{y} + 2X^T X \\vec{w}\\right) \\end{array}$$\n",
    "$$\\large \\begin{array}{rcl} \\frac{\\partial \\mathcal{L}}{\\partial \\vec{w}} = 0 &\\Leftrightarrow& \\frac{1}{2n} \\left(-2 X^T \\vec{y} + 2X^T X \\vec{w}\\right) = 0 \\\\ &\\Leftrightarrow& -X^T \\vec{y} + X^T X \\vec{w} = 0 \\\\ &\\Leftrightarrow& X^T X \\vec{w} = X^T \\vec{y} \\\\ &\\Leftrightarrow& \\vec{w} = \\left(X^T X\\right)^{-1} X^T \\vec{y} \\end{array}$$\n",
    "\n",
    "Итак, имея в виду все определения и условия описанные выше, мы можем утверждать, опираясь на теорему Маркова-Гаусса, что оценка МНК является лучшей оценкой параметров модели, среди всех линейных и несмещенных оценок, то есть обладающей наименьшей дисперсией."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Maximum Likelihood Estimation\n",
    "**Метод максимального правдоподобия** — это способ оценки параметров статистической модели, при котором выбираются такие параметры, которые максимизируют вероятность наблюдаемых данных.\n",
    "\n",
    "Применим MLE для задачи линейной регрессии и попробуем выяснить, что лежит за среднеквадратичной ошибкой. Будем считать, что случайные ошибки берустся из нормального распределения. $\\large \\epsilon_i \\sim \\mathcal{N}\\left(0, \\sigma^2\\right)$\n",
    "\n",
    "$y_i = \\sum_{j=1}^{m}w_jX_{ij} + \\epsilon_i \\sim \\sum_{j=1}^{m}w_jX_{ij} + \\mathcal{N}(0, \\sigma^2) $\n",
    "\n",
    "$p(y_i | X; w) = \\mathcal{N}(\\sum_{j=1}^{m}w_jX_{ij}, \\sigma^2)$\n",
    "\n",
    "Так как примеры независимы (ошибки не скоррелированы – одно из условий теоремы Маркова-Гаусса), то полное распределение будет произведением всех $p(y_i)$\n",
    "\n",
    "$ \\begin{array}{rcl} \\log p\\left(\\vec{y} \\mid X, \\vec{w}\\right) &=& \\log \\prod_{i=1}^n \\mathcal{N}\\left(\\sum_{j=1}^m w_j X_{ij}, \\sigma^2\\right) \\\\ &=& \\sum_{i=1}^n \\log \\mathcal{N}\\left(\\sum_{j=1}^m w_j X_{ij}, \\sigma^2\\right) \\\\ &=& -\\frac{n}{2}\\log 2\\pi\\sigma^2 -\\frac{1}{2\\sigma^2} \\sum_{i=1}^n \\left(y_i - \\vec{w}^T \\vec{x}_i\\right)^2 \\end{array}$\n",
    "\n",
    "Чтобы найти гипотезу максимального правдоподобия, нужно максимизировать $p\\left(\\vec{y} \\mid X, \\vec{w}\\right)$\n",
    "\n",
    "$ \\begin{array}{rcl} \\hat{w} &=& \\arg \\max_{w} p\\left(\\vec{y} \\mid X, \\vec{w}\\right) \\\\ &=& \\arg \\max_{w} -\\frac{n}{2}\\log 2\\pi\\sigma^2 -\\frac{1}{2\\sigma^2} \\sum_{i=1}^n \\left(y_i - \\vec{w}^T \\vec{x}_i\\right)^2 \\\\ &=& \\arg \\max_{w} -\\frac{1}{2\\sigma^2} \\sum_{i=1}^n \\left(y_i - \\vec{w}^T \\vec{x}_i\\right)^2 \\\\ &=& \\arg \\max_{w} -\\mathcal{L}\\left(X, \\vec{y}, \\vec{w} \\right) \\end{array}$\n",
    "\n",
    "**Вывод:** максимизация правдаподобия данных, тоже самое, что и минимизация среднеквадратичной ошибки. Получается, что именно такая функция стоймости является следствием того, что ошибка распределеня нормально."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias-Variance Decomposition\n",
    "**Разложение ошибки на смещение и разброс**\n",
    "\n",
    "Свойства ошибки прогноза линейной регрессии\n",
    "\n",
    "- истинное значение целевой переменной складывается из некоторой детерминированной функции $f\\left(\\vec{x}\\right)$ и случайной ошибки $\\epsilon$: $y = f\\left(\\vec{x}\\right) + \\epsilon$;\n",
    "- ошибка распределена нормально с центром в нуле и некоторым разбросом: $\\epsilon \\sim \\mathcal{N}\\left(0, \\sigma^2\\right)$;\n",
    "- истинное значение целевой переменной тоже распределено нормально: $y \\sim \\mathcal{N}\\left(f\\left(\\vec{x}\\right), \\sigma^2\\right)$\n",
    "- мы пытаемся приблизить детерминированную, но неизвестную функцию $f\\left(\\vec{x}\\right)$ линейной функцией от регрессоров $\\hat{f}\\left(\\vec{x}\\right)$, которая, в свою очередь, является точечной оценкой функции $f$ в - - пространстве функций (точнее, мы ограничили пространство функций параметрическим семейством линейных функций), т.е. случайной переменной, у которой есть среднее значение и дисперсия.\n",
    "  \n",
    "Тогда ошибка в точке $\\vec{x}$ раскладывается следующим образом:\n",
    "\n",
    "$ \\begin{array}{rcl} \\text{Err}\\left(\\vec{x}\\right) &=& \\mathbb{E}\\left[\\left(y - \\hat{f}\\left(\\vec{x}\\right)\\right)^2\\right] \\\\ &=& \\mathbb{E}\\left[y^2\\right] + \\mathbb{E}\\left[\\left(\\hat{f}\\left(\\vec{x}\\right)\\right)^2\\right] - 2\\mathbb{E}\\left[y\\hat{f}\\left(\\vec{x}\\right)\\right] \\\\ &=& \\mathbb{E}\\left[y^2\\right] + \\mathbb{E}\\left[\\hat{f}^2\\right] - 2\\mathbb{E}\\left[y\\hat{f}\\right] \\\\ \\end{array}$\n",
    "\n",
    " Рассмотрим каждый член в отдельности, первые два расписываются легко по формуле $\\text{Var}\\left(z\\right) = \\mathbb{E}\\left[z^2\\right] - \\mathbb{E}\\left[z\\right]^2$:\n",
    "\n",
    " $\\large \\begin{array}{rcl} \\mathbb{E}\\left[y^2\\right] &=& \\text{Var}\\left(y\\right) + \\mathbb{E}\\left[y\\right]^2 = \\sigma^2 + f^2\\\\ \\mathbb{E}\\left[\\hat{f}^2\\right] &=& \\text{Var}\\left(\\hat{f}\\right) + \\mathbb{E}\\left[\\hat{f}\\right]^2 \\\\ \\end{array}$\n",
    "\n",
    " $\\large \\begin{array}{rcl} \\mathbb{E}\\left[y\\hat{f}\\right] &=& \\mathbb{E}\\left[\\left(f + \\epsilon\\right)\\hat{f}\\right] \\\\ &=& \\mathbb{E}\\left[f\\hat{f}\\right] + \\mathbb{E}\\left[\\epsilon\\hat{f}\\right] \\\\ &=& f\\mathbb{E}\\left[\\hat{f}\\right] + \\mathbb{E}\\left[\\epsilon\\right] \\mathbb{E}\\left[\\hat{f}\\right] = f\\mathbb{E}\\left[\\hat{f}\\right] \\end{array}$\n",
    "\n",
    " Итак:\n",
    "\n",
    " $\\large \\begin{array}{rcl} \\text{Err}\\left(\\vec{x}\\right) &=& \\mathbb{E}\\left[\\left(y - \\hat{f}\\left(\\vec{x}\\right)\\right)^2\\right] \\\\ &=& \\sigma^2 + f^2 + \\text{Var}\\left(\\hat{f}\\right) + \\mathbb{E}\\left[\\hat{f}\\right]^2 - 2f\\mathbb{E}\\left[\\hat{f}\\right] \\\\ &=& \\left(f - \\mathbb{E}\\left[\\hat{f}\\right]\\right)^2 + \\text{Var}\\left(\\hat{f}\\right) + \\sigma^2 \\\\ &=& \\text{Bias}\\left(\\hat{f}\\right)^2 + \\text{Var}\\left(\\hat{f}\\right) + \\sigma^2 \\end{array}$\n",
    "\n",
    " Это значит, что ошибка прогноза любой модели вида $y = f\\left(\\vec{x}\\right) + \\epsilon$ складывается из:\n",
    " - квадрата смещения: $\\text{Bias}\\left(\\hat{f}\\right)$ – средняя ошибка по всевозможным наборам данных;\n",
    " - дисперсии: $\\text{Var}\\left(\\hat{f}\\right)$ – вариативность ошибки, то, на сколько ошибка будет отличаться, если обучать модель на разных наборах данных;\n",
    " - неустранимой ошибки: $\\sigma^2$.\n",
    "\n",
    "Если с последней мы ничего сделать не можем, то на первые два слагаемых мы можем как-то влиять. В идеале, конечно же, хотелось бы свести на нет оба этих слагаемых (левый верхний квадрат рисунка), но на практике часто приходится балансировать между смещенными и нестабильными оценками (высокая дисперсия).\n",
    "\n",
    "<img src=\"https://mlcourse.ai/_images/topic4_bias_variance.png\" width=\"400\">\n",
    "\n",
    "С увеличением сложности модели уменьшается смещение, но растёт дисперсия. Простые модели недоучиваются (недообучение), а слишком сложные запоминают данные вместо обобщения (переобучение).\n",
    "\n",
    "<img src=\"https://habrastorage.org/r/w1560/files/fac/875/6ad/fac8756ad9e64cae85cd2f2b2c289d05.png\" width=\"400\">\n",
    "\n",
    "Теорема Маркова-Гаусса утверждает, что среди всех линейных несмещённых оценок параметров линейной модели метод наименьших квадратов (МНК) обладает наименьшей дисперсией. Это означает, что если существует другая линейная несмещенная модель, то её дисперсия будет как минимум не меньше, чем у МНК-оценки. Формально, для любой другой линейной несмещённой модели \n",
    "$$\\text{Var}(\\hat f) \\leq \\text{Var}( g)$$\n",
    "​\t\n",
    " Где $\\hat f$ — это оценка, полученная методом наименьших квадратов. То есть, МНК-оценка является наиболее \"точной\" (с наименьшей изменчивостью) среди всех возможных линейных оценок."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization of Linear Regression\n",
    "\n",
    "В некоторых случаях мы намерено увеличиваем смещение модели, в угоду стабильности, т.е. ради уменьшенгия дисперсии модели $Var\\left(\\hat{f}\\right) \\leq Var\\left(g\\right)$.\n",
    "Одним из условий теоремы Маркова-Гауса является полный столбцовый ранг матрицы $X$. Иначе решение OLS $\\vec{w} = \\left(X^T X\\right)^{-1} X^T \\vec{y}$ не существует, т.к. не будет существовать обратная матрица $(X^T X)^{-1}$. Такая задача будет называться некоректно поставленной. Задачу можно скоиектровать, а именно, сделать $X^TX$ невырожденной, или регулярной (поэтому процесс и называется регулязацией).\n",
    "\n",
    "Чаще в данных мы можем наблюдать так называемую мультиколлинеарность — когда два или несколько признаков сильно коррелированы, в матрице $X$ это проявляется в виде \"почти\" линейной зависимости столбцов.\n",
    "\n",
    "Формально для таких данных матрица $X^T X$ будет обратима, но из-за мультиколлинеарности у матрицы $X^T X$ некоторые собственные значения будут близки к нулю, а в обратной матрице $\\left(X^T X\\right)^{-1}$ появятся экстремально большие собственные значения, т.к. собственные значения обратной матрицы – это $\\frac{1}{\\lambda_i}$. \n",
    "\n",
    "Итогом такого шатания собственных значений станет нестабильная оценка параметров модели, то есть добавление нового наблюдения в набор данных приведет к другому решению.\n",
    "\n",
    "Одним из способов регуляризации является **регуляризация Тихонова**, которая в общем виде выглядит как добавление нового члена к среднеквадратичной ошибке:\n",
    "\n",
    "$\\large \\begin{array}{rcl} \\mathcal{L}\\left(X, \\vec{y}, \\vec{w} \\right) &=& \\frac{1}{2n} \\left\\| \\vec{y} - X \\vec{w} \\right\\|_2^2 + \\left\\|\\Gamma \\vec{w}\\right\\|^2\\\\ \\end{array}$\n",
    "\n",
    "Часто матрица Тихонова выражается как произведение некоторого числа на единичную матрицу: $\\Gamma = \\frac{\\lambda}{2} E$. В этом случае задача минимизации среднеквадратичной ошибки становится задачей с ограничением на $L_2$ норму. Если продифференцировать новую функцию стоимости по параметрам модели, приравнять полученную функцию к нулю и выразить $\\vec{w}$, то мы получим точное решение задачи.\n",
    "$ \\begin{array}{rcl} \\vec{w} &=& \\left(X^T X + \\lambda E\\right)^{-1} X^T \\vec{y} \\end{array}$\n",
    "\n",
    "Такая регрессия называется гребневой регрессией (ridge regression). А гребнем является как раз диагональная матрица, которую мы прибавляем\n",
    "\n",
    "<img src=\"https://habrastorage.org/r/w1560/files/c6e/21e/c4d/c6e21ec4d5364b59a727c193760a40e1.png\" width=\"400\">\n",
    "\n",
    "Такое решение уменьшает дисперсию, но становится смещенным, т.к. минимизируется также и норма вектора параметров, что заставляет решение сдвигаться в сторону нуля."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
